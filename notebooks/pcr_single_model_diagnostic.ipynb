{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d25069b",
   "metadata": {},
   "source": [
    "\n",
    "# PCR Single-Model Diagnostic (30m Train, 6m Test)\n",
    "\n",
    "Goal:\n",
    "- Diagnose why `baseline + PCR` degraded in recent workflow.\n",
    "- Train/test a single ticker model with **30 months train** and **6 months test**.\n",
    "- Compare against residual baseline (`0.0`) and inspect model stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = Path('data') if Path('data/raw/adr_info.csv').exists() else Path('../data')\n",
    "MODEL_DIR = DATA_DIR / 'processed' / 'models' / 'with_us_stocks' / 'pcr'\n",
    "FEAT_DIR = DATA_DIR / 'processed' / 'models' / 'with_us_stocks' / 'features_extended'\n",
    "SIGNAL_DIR = DATA_DIR / 'processed' / 'index_russell_pcr_signal'\n",
    "BASELINE_DIR = DATA_DIR / 'processed' / 'futures_only_signal'\n",
    "\n",
    "assert MODEL_DIR.exists()\n",
    "assert FEAT_DIR.exists()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6717e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_ic(pred, y):\n",
    "    a = np.asarray(pred, dtype=float)\n",
    "    b = np.asarray(y, dtype=float)\n",
    "    m = np.isfinite(a) & np.isfinite(b)\n",
    "    if m.sum() < 5:\n",
    "        return np.nan\n",
    "    a = a[m]\n",
    "    b = b[m]\n",
    "    if np.std(a) == 0 or np.std(b) == 0:\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(a, b)[0, 1])\n",
    "\n",
    "\n",
    "def fit_pcr_with_val(X_train, y_train, X_val, y_val, comp_grid):\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    Xt = scaler.transform(X_train)\n",
    "    Xv = scaler.transform(X_val)\n",
    "\n",
    "    max_comp = int(min(Xt.shape[0], Xt.shape[1]))\n",
    "    grid = sorted(set([int(k) for k in comp_grid if 1 <= int(k) <= max_comp]))\n",
    "    if not grid:\n",
    "        grid = [min(40, max_comp)]\n",
    "\n",
    "    pca_all = PCA(n_components=max(grid), svd_solver='randomized', random_state=42)\n",
    "    Zt_all = pca_all.fit_transform(Xt)\n",
    "    Zv_all = pca_all.transform(Xv)\n",
    "    zt_norm2 = np.sum(Zt_all * Zt_all, axis=0)\n",
    "    zt_norm2 = np.where(zt_norm2 <= 1e-12, 1e-12, zt_norm2)\n",
    "\n",
    "    best = (-np.inf, None)\n",
    "    y_train = np.asarray(y_train, dtype=float)\n",
    "    for k in grid:\n",
    "        beta = (Zt_all[:, :k].T @ y_train) / zt_norm2[:k]\n",
    "        pred = Zv_all[:, :k] @ beta\n",
    "        ic = compute_ic(pred, y_val)\n",
    "        if np.isfinite(ic) and ic > best[0]:\n",
    "            best = (ic, int(k))\n",
    "\n",
    "    # Refit on train+val with chosen k\n",
    "    Xfull = np.vstack([X_train, X_val])\n",
    "    yfull = np.concatenate([y_train, y_val])\n",
    "    scaler = StandardScaler().fit(Xfull)\n",
    "    Xf = scaler.transform(Xfull)\n",
    "    k = best[1] if best[1] is not None else min(40, Xf.shape[1], Xf.shape[0])\n",
    "    k = int(max(1, min(k, Xf.shape[0], Xf.shape[1])))\n",
    "\n",
    "    pca = PCA(n_components=k, svd_solver='randomized', random_state=42)\n",
    "    Zf = pca.fit_transform(Xf)\n",
    "    zf_norm2 = np.sum(Zf * Zf, axis=0)\n",
    "    zf_norm2 = np.where(zf_norm2 <= 1e-12, 1e-12, zf_norm2)\n",
    "    beta = (Zf.T @ np.asarray(yfull, dtype=float)) / zf_norm2\n",
    "\n",
    "    w_scaled = pca.components_.T @ beta.astype(np.float32)\n",
    "    scale = np.where(scaler.scale_.astype(np.float32) <= 1e-6, 1e-6, scaler.scale_.astype(np.float32))\n",
    "    mean = scaler.mean_.astype(np.float32)\n",
    "    w_raw = w_scaled / scale\n",
    "    c_raw = -float(np.dot(mean / scale, w_scaled))\n",
    "\n",
    "    return {\n",
    "        'val_ic': best[0],\n",
    "        'n_components': k,\n",
    "        'w_raw': w_raw,\n",
    "        'c_raw': c_raw,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b5f97",
   "metadata": {},
   "source": [
    "## 1) Artifact stability check (recent months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef431df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_model_row(ticker, pkl_path):\n",
    "    md = pickle.load(open(pkl_path, 'rb'))\n",
    "    w = np.asarray(md.get('w_raw', []), dtype=float)\n",
    "    return {\n",
    "        'ticker': ticker,\n",
    "        'model_file': pkl_path.name,\n",
    "        'model_date': md.get('model_date'),\n",
    "        'n_components': md.get('n_components'),\n",
    "        'val_ic': md.get('val_ic'),\n",
    "        'test_ic': md.get('test_ic'),\n",
    "        'w_max_abs': float(np.nanmax(np.abs(w))) if w.size else np.nan,\n",
    "        'c_raw': md.get('c_raw'),\n",
    "        'n_features': len(md.get('feature_names', [])),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for ticker in ['ASML', 'NVO', 'AEG', 'BABA']:\n",
    "    tdir = MODEL_DIR / ticker\n",
    "    for p in sorted(tdir.glob('2025_*.pkl')) + sorted(tdir.glob('2026_*.pkl')):\n",
    "        rows.append(read_model_row(ticker, p))\n",
    "\n",
    "artifact_df = pd.DataFrame(rows)\n",
    "artifact_df['model_date'] = pd.to_datetime(artifact_df['model_date'])\n",
    "artifact_df = artifact_df.sort_values(['ticker', 'model_date'])\n",
    "artifact_df.tail(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b08851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Highlight unstable models by coefficient scale\n",
    "unstable = artifact_df[artifact_df['w_max_abs'] > 1e6].copy()\n",
    "print('Unstable rows (w_max_abs > 1e6):', len(unstable))\n",
    "unstable[['ticker', 'model_file', 'model_date', 'n_components', 'val_ic', 'test_ic', 'w_max_abs']].head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7a001",
   "metadata": {},
   "source": [
    "## 2) Single-model train/test (30m train, 6m test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff8dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Diagnostic split around weak period\n",
    "TRAIN_START = pd.Timestamp('2023-03-01')\n",
    "TRAIN_END = pd.Timestamp('2025-08-31')\n",
    "TEST_START = pd.Timestamp('2025-09-01')\n",
    "TEST_END = pd.Timestamp('2026-02-28')\n",
    "\n",
    "# Internal validation from the last 6 months of train period\n",
    "VAL_START = pd.Timestamp('2025-03-01')\n",
    "VAL_END = pd.Timestamp('2025-08-31')\n",
    "CORE_TRAIN_START = TRAIN_START\n",
    "CORE_TRAIN_END = pd.Timestamp('2025-02-28')\n",
    "\n",
    "COMP_GRIDS = {\n",
    "    'cap40': [1,2,5,10,20,40],\n",
    "    'cap80': [1,2,5,10,20,40,80],\n",
    "    'cap120': [1,2,5,10,20,40,80,120],\n",
    "    'cap200': [1,2,5,10,20,40,80,120,200],\n",
    "    'aggressive': [1,2,5,10,20,40,80,120,200,400,800],\n",
    "}\n",
    "\n",
    "TICKERS = ['ASML', 'NVO']\n",
    "\n",
    "results = []\n",
    "for ticker in TICKERS:\n",
    "    f = FEAT_DIR / f'{ticker}.parquet'\n",
    "    df = pd.read_parquet(f)\n",
    "    dt = pd.to_datetime(df['date']) if 'date' in df.columns else pd.to_datetime(df.index)\n",
    "    feature_cols = [c for c in df.columns if c.startswith('russell_')]\n",
    "\n",
    "    X = df[feature_cols].to_numpy(dtype=float)\n",
    "    y = df['ordinary_residual'].to_numpy(dtype=float)\n",
    "\n",
    "    m_core = (dt >= CORE_TRAIN_START) & (dt <= CORE_TRAIN_END)\n",
    "    m_val = (dt >= VAL_START) & (dt <= VAL_END)\n",
    "    m_test = (dt >= TEST_START) & (dt <= TEST_END)\n",
    "\n",
    "    Xtr, ytr = X[m_core], y[m_core]\n",
    "    Xva, yva = X[m_val], y[m_val]\n",
    "    Xte, yte = X[m_test], y[m_test]\n",
    "\n",
    "    baseline_test_ic = compute_ic(np.zeros_like(yte), yte)\n",
    "\n",
    "    for label, grid in COMP_GRIDS.items():\n",
    "        fit = fit_pcr_with_val(Xtr, ytr, Xva, yva, grid)\n",
    "        pred_test = Xte @ fit['w_raw'] + fit['c_raw']\n",
    "        test_ic = compute_ic(pred_test, yte)\n",
    "\n",
    "        results.append({\n",
    "            'ticker': ticker,\n",
    "            'config': label,\n",
    "            'n_components': fit['n_components'],\n",
    "            'val_ic': fit['val_ic'],\n",
    "            'test_ic': test_ic,\n",
    "            'baseline_test_ic': baseline_test_ic,\n",
    "            'improvement_vs_baseline': test_ic - baseline_test_ic,\n",
    "            'w_max_abs': float(np.nanmax(np.abs(fit['w_raw']))),\n",
    "            'pred_test_std': float(np.nanstd(pred_test)),\n",
    "        })\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(['ticker', 'improvement_vs_baseline'], ascending=[True, False])\n",
    "res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d0096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for t in TICKERS:\n",
    "    d = res_df[res_df['ticker'] == t].sort_values('improvement_vs_baseline', ascending=False)\n",
    "    print('\n",
    "Ticker:', t)\n",
    "    display(d[['config','n_components','val_ic','test_ic','baseline_test_ic','improvement_vs_baseline','w_max_abs','pred_test_std']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e761db",
   "metadata": {},
   "source": [
    "## 3) Inference signal sanity (current pipeline outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98739964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sig_rows = []\n",
    "for ticker in TICKERS:\n",
    "    p = SIGNAL_DIR / f'ticker={ticker}' / 'data.parquet'\n",
    "    if not p.exists():\n",
    "        continue\n",
    "    s = pd.read_parquet(p)\n",
    "    if s.empty:\n",
    "        continue\n",
    "    s.index = pd.to_datetime(s.index)\n",
    "    m = s.index >= pd.Timestamp('2025-09-01', tz='America/New_York')\n",
    "    x = s.loc[m, 'signal'].astype(float)\n",
    "    if x.empty:\n",
    "        continue\n",
    "    sig_rows.append({\n",
    "        'ticker': ticker,\n",
    "        'n': len(x),\n",
    "        'abs_q50': float(np.nanpercentile(np.abs(x), 50)),\n",
    "        'abs_q95': float(np.nanpercentile(np.abs(x), 95)),\n",
    "        'abs_q99': float(np.nanpercentile(np.abs(x), 99)),\n",
    "        'abs_q999': float(np.nanpercentile(np.abs(x), 99.9)),\n",
    "        'min': float(np.nanmin(x)),\n",
    "        'max': float(np.nanmax(x)),\n",
    "    })\n",
    "\n",
    "pd.DataFrame(sig_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f023dcc",
   "metadata": {},
   "source": [
    "\n",
    "## Interpretation guide\n",
    "\n",
    "- If `w_max_abs` is huge and `pred_test_std` is huge under aggressive component settings, that indicates numerical instability.\n",
    "- If capped-component configs keep `w_max_abs` and `pred_test_std` in normal ranges while preserving or improving test IC, that suggests a clear mitigation path:\n",
    "  1. cap PCR components,\n",
    "  2. floor tiny PCA/scale denominators,\n",
    "  3. optionally add signal clipping / robust training.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
